{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "9778edb3",
   "metadata": {
    "cellId": "8oyzx5gce132jmtjxkds1a"
   },
   "outputs": [
    {
     "ename": "Execute error",
     "evalue": "Servant g1.1 not allocated: Internal Error",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "#!g1.1\n",
    "#%pip install opencv-python editdistance ml_collections wandb albumentations\n",
    "#%pip install bezier\n",
    "#%pip install typing_extensions --upgrade\n",
    "#%pip install pandas -U\n",
    "%pip install numpy==1.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "21dbb268",
   "metadata": {
    "cellId": "4hsdt4pafaqchqmpp23kdc"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import torchvision as tv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import typing as tp\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "from editdistance import eval as edit_distance\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import errno\n",
    "\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "import importlib\n",
    "import wandb\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "ba876e59",
   "metadata": {
    "cellId": "4vhwwnmr9ein4iqii8pwbo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from diploma_code.patched_hwb import HandWrittenBlot\n",
    "\n",
    "# from diploma_code.model import (\n",
    "#     ConvBlock, FusedInvertedBottleneck, ReduceBlock, Backbone, PositionalEncoding,\n",
    "#     TransformerEncoder, CTCRawDecoder, CTCDecoderModel, ParallelModel, make_single_model, make_model\n",
    "# )\n",
    "\n",
    "from diploma_code.model_v2 import (\n",
    "    Resnet34Backbone, BiLSTMEncoder, PositionalEncoding, CoolTransformerEncoderLayer, TransformerEncoder, \n",
    "    CTCDecoderModel, ParallelModel, make_single_model_v2, make_model_v2\n",
    ")\n",
    "from diploma_code.transforms_torch import (\n",
    "    VerticalRandomMasking, HorizontalResizeOnly, GaussianNoise, \n",
    "    RandomHorizontalStretch, RandomChoiceN\n",
    ")\n",
    "from diploma_code.char_encoder import (\n",
    "    CharEncoder\n",
    ")\n",
    "# from diploma_code.data_loader.mjsynth import (\n",
    "#     load_mjsynth_chars, load_mjsynth_samples\n",
    "# )\n",
    "from diploma_code.dataset import (\n",
    "    BaseLTRDataset, LongLinesLTRDataset\n",
    ")\n",
    "from diploma_code.make_loader import (\n",
    "    make_char_encoder, make_datasets, make_dataloaders\n",
    ")\n",
    "from diploma_code.optimizing import (\n",
    "    pytorch_make_optimizer, StepLRWithWarmup, ReverseSqrtWithLinearWarmup, \n",
    "    ConstantLambdaLR, make_lr_scheduler_torch, make_lr_scheduler_lambda\n",
    ")\n",
    "from diploma_code.utils import (\n",
    "    log_metric_wandb, batch_to_device, seed_everything\n",
    ")\n",
    "from diploma_code.evaluation import (\n",
    "    my_ctc_loss, my_dml_loss, my_mcl_loss, my_dml_loss_tr, \n",
    "    trkl_div, decode_ocr_probs, get_edit_distance, \n",
    "    EpochValueProcessor, EpochDMLProcessor, CERProcessor\n",
    ")\n",
    "from diploma_code.trainer import (\n",
    "    LTRTrainer\n",
    ")\n",
    "\n",
    "from diploma_code.config import default_diploma_config, get_config\n",
    "\n",
    "from diploma_code.configs import BaseDatasetConfig\n",
    "from diploma_code.configs import IamConfig\n",
    "from diploma_code.configs import SaintgallConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "28c67e71",
   "metadata": {
    "cellId": "dsyur67hhd4p3ycqwbpvd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'diploma_code.configs' from '/home/jupyter/work/resources/mutual_htr/mutual_htr/diploma_code/configs/__init__.py'>"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "import diploma_code\n",
    "import diploma_code.data_loader\n",
    "\n",
    "importlib.reload(diploma_code.char_encoder)\n",
    "\n",
    "importlib.reload(diploma_code.config)\n",
    "importlib.reload(diploma_code.dataset)\n",
    "\n",
    "importlib.reload(diploma_code.data_loader)\n",
    "importlib.reload(diploma_code.evaluation)\n",
    "\n",
    "importlib.reload(diploma_code.optimizing)\n",
    "\n",
    "importlib.reload(diploma_code.model_v2)\n",
    "importlib.reload(diploma_code.utils)\n",
    "importlib.reload(diploma_code.trainer)\n",
    "\n",
    "importlib.reload(diploma_code.make_loader)\n",
    "importlib.reload(diploma_code.make_transforms)\n",
    "\n",
    "importlib.reload(diploma_code.transforms_functional)\n",
    "\n",
    "importlib.reload(diploma_code.configs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "099bd869",
   "metadata": {
    "cellId": "9l4jwd26tal3yxxoouvwtn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkafka_zhuk\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "fee39d96",
   "metadata": {
    "cellId": "lkqpvvzr28zlsz3tfaea"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "cfg = get_config()\n",
    "cfg.training.eval_epochs_interval = 3\n",
    "cfg.training.eval_test_interval = 3\n",
    "cfg.training.snapshot_epochs_interval = 3\n",
    "cfg.data.root_path = '/home/jupyter/mnt/datasets/diploma/'\n",
    "cfg.data.iam.path = '/home/jupyter/mnt/datasets/diploma/'\n",
    "cfg.data.saintgall.path = '/home/jupyter/mnt/datasets/diploma'\n",
    "\n",
    "cfg.data.dataset = 'iam'\n",
    "\n",
    "cfg.model.type = 'single'\n",
    "# cfg.model.first.backbone.Resnet34Backbone.pretrained = True\n",
    "# cfg.model.first.encoder.constructor = 'TransformerEncoder'\n",
    "# cfg.model.first.encoder.TransformerEncoder.num_layers = 4\n",
    "# cfg.model.first.encoder.TransformerEncoder.num_heads = 4\n",
    "# cfg.model.first.encoder.TransformerEncoder.hidden_features = 256\n",
    "\n",
    "# cfg.model.second.backbone.Resnet34Backbone.pretrained = True\n",
    "# cfg.model.second.encoder.constructor = 'TransformerEncoder'\n",
    "# cfg.model.second.encoder.TransformerEncoder.num_layers = 4\n",
    "# cfg.model.second.encoder.TransformerEncoder.num_heads = 4\n",
    "# cfg.model.second.encoder.TransformerEncoder.hidden_features = 256\n",
    "\n",
    "cfg.device = 'cuda:0'\n",
    "\n",
    "cfg.training.batch_size = 16\n",
    "cfg.training.grad_clip_value = 1\n",
    "cfg.training.num_epochs = 100\n",
    "\n",
    "# cfg.lr_scheduler.use_lambda = True\n",
    "# cfg.lr_scheduler.constructor = \"ReverseSqrtWithLinearWarmup\"\n",
    "# cfg.lr_scheduler.params = ConfigDict()\n",
    "# cfg.lr_scheduler.params.warmup_steps = 3000\n",
    "\n",
    "cfg.lr_scheduler.params.epochs = 100\n",
    "cfg.lr_scheduler.params.max_lr = 1e-3\n",
    "cfg.lr_scheduler.params.steps_per_epoch = (cfg.data.get_ref(cfg.data.dataset).get().get_ref('length') + cfg.training.get_ref('batch_size') - 1) // cfg.training.get_ref('batch_size')\n",
    "\n",
    "cfg.backbone_lr_scheduler.params.epochs = 100\n",
    "cfg.backbone_lr_scheduler.params.max_lr = 1e-3\n",
    "cfg.backbone_lr_scheduler.params.steps_per_epoch = (cfg.data.get_ref(cfg.data.dataset).get().get_ref('length') + cfg.training.get_ref('batch_size') - 1) // cfg.training.get_ref('batch_size')\n",
    "\n",
    "cfg.optimizer.weight_decay = 0.01\n",
    "#cfg.optimizer.params.lr = 1e-5\n",
    "\n",
    "cfg.backbone_optimizer.weight_decay = 0.01\n",
    "#cfg.backbone_optimizer.params.lr = 1e-5\n",
    "\n",
    "#cfg.training.checkpoints_folder = './checkpoints_single_lstm_67_60ep_sg'\n",
    "cfg.training.checkpoints_folder = './checkpoints_single_lstm_42_200_epoch'\n",
    "cfg.load_checkpoints_folder = './checkpoints_single_lstm_42_200_epoch'\n",
    "\n",
    "cfg.training.load_from_checkpoint = True\n",
    "\n",
    "cfg.eval = True\n",
    "#cfg.seed = 42\n",
    "cfg.seed = 67\n",
    "#cfg.seed = 95\n",
    "\n",
    "cfg.data.saintgall.chars = ' !\"#&\\'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "# 3 epoch warmup\n",
    "\n",
    "#cfg.dml.warmup_steps = 3 * (cfg.data.get_ref(cfg.data.dataset).get().get_ref('length') + cfg.training.get_ref('batch_size') - 1) // cfg.training.get_ref('batch_size')\n",
    "cfg.dml.warmup_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "4b35baff",
   "metadata": {
    "cellId": "3lt4s0p3abx29tfud0thpd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backbone_lr_scheduler:\n",
       "  constructor: torch.optim.lr_scheduler.OneCycleLR\n",
       "  params:\n",
       "    anneal_strategy: cos\n",
       "    epochs: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "      _field_type: &id001 !!python/name:builtins.int ''\n",
       "      _ops: []\n",
       "      _required: false\n",
       "      _value: 100\n",
       "    final_div_factor: 100000\n",
       "    max_lr: 0.001\n",
       "    pct_start: 0.1\n",
       "    steps_per_epoch: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "      _field_type: *id001\n",
       "      _ops: []\n",
       "      _required: false\n",
       "      _value: 604\n",
       "  use_lambda: false\n",
       "backbone_optimizer:\n",
       "  constructor: torch.optim.AdamW\n",
       "  params:\n",
       "    betas: &id008 !!python/tuple\n",
       "    - 0.9\n",
       "    - 0.999\n",
       "    lr: 0.0002\n",
       "  weight_decay: 0.01\n",
       "data:\n",
       "  dataset: iam\n",
       "  iam: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "    _field_type: !!python/name:builtins.dict ''\n",
       "    _ops: []\n",
       "    _required: false\n",
       "    _value:\n",
       "      blank: \"\\u03B2\"\n",
       "      chars: ' !\"#&''()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
       "      config_constructor: diploma_code.configs.IamConfig\n",
       "      image_height: &id002 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "        _field_type: *id001\n",
       "        _ops: []\n",
       "        _required: false\n",
       "        _value: 64\n",
       "      image_width: &id003 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "        _field_type: *id001\n",
       "        _ops: []\n",
       "        _required: false\n",
       "        _value: 1024\n",
       "      length: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "        _field_type: *id001\n",
       "        _ops: []\n",
       "        _required: false\n",
       "        _value: 9652\n",
       "      path: /home/jupyter/mnt/datasets/diploma/\n",
       "      train_dataset_constructor: BaseLTRDataset\n",
       "  image_height: *id002\n",
       "  image_width: *id003\n",
       "  root_path: /home/jupyter/mnt/datasets/diploma/\n",
       "  saintgall:\n",
       "    blank: \"\\u03B2\"\n",
       "    chars: ' !\"#&''()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
       "    config_constructor: diploma_code.configs.SaintgallConfig\n",
       "    image_height: *id002\n",
       "    image_width: *id003\n",
       "    length: 468\n",
       "    path: /home/jupyter/mnt/datasets/diploma\n",
       "    train_dataset_constructor: BaseLTRDataset\n",
       "  transforms:\n",
       "    basic_albums:\n",
       "      CLAHE:\n",
       "        enabled: true\n",
       "        params:\n",
       "          always_apply: false\n",
       "          clip_limit: 4.0\n",
       "          p: 0.25\n",
       "          tile_grid_size: !!python/tuple\n",
       "          - 8\n",
       "          - 8\n",
       "      ImageCompression:\n",
       "        enabled: true\n",
       "        params:\n",
       "          p: 0.5\n",
       "          quality_lower: 75\n",
       "      Rotate:\n",
       "        enabled: true\n",
       "        params:\n",
       "          border_mode: 0\n",
       "          interpolation: 1\n",
       "          limit: 2\n",
       "          p: 0.5\n",
       "    blot:\n",
       "      enabled: true\n",
       "      p: 0.5\n",
       "      params:\n",
       "        params:\n",
       "          count: !!python/tuple\n",
       "          - 1\n",
       "          - 10\n",
       "          incline: !!python/tuple\n",
       "          - 10\n",
       "          - 50\n",
       "          intensivity: !!python/tuple\n",
       "          - 0.75\n",
       "          - 0.75\n",
       "          transparency: !!python/tuple\n",
       "          - 0.05\n",
       "          - 0.4\n",
       "        rect_config:\n",
       "          h: !!python/tuple\n",
       "          - 25\n",
       "          - 50\n",
       "          w: !!python/tuple\n",
       "          - 20\n",
       "          - 60\n",
       "          x: &id004 !!python/tuple\n",
       "          - null\n",
       "          - null\n",
       "          y: *id004\n",
       "device: cuda:0\n",
       "dml:\n",
       "  max_weight: 1.0\n",
       "  min_weight: 0.0\n",
       "  warmup_steps: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "    _field_type: *id001\n",
       "    _ops: []\n",
       "    _required: false\n",
       "    _value: 0\n",
       "eval: true\n",
       "evaluate:\n",
       "  batch_size: 16\n",
       "  load_from_checkpoint: true\n",
       "  loader_num_workers: 6\n",
       "global_vars:\n",
       "  hidden_features: &id007 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "    _field_type: *id001\n",
       "    _ops: []\n",
       "    _required: false\n",
       "    _value: 256\n",
       "  image_height: *id002\n",
       "  image_width: *id003\n",
       "  num_classes: &id005 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "    _field_type: *id001\n",
       "    _ops: []\n",
       "    _required: false\n",
       "    _value: 81\n",
       "  time_feature_count: &id006 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "    _field_type: *id001\n",
       "    _ops: []\n",
       "    _required: false\n",
       "    _value: 256\n",
       "load_checkpoints_folder: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "  _field_type: &id009 !!python/name:builtins.str ''\n",
       "  _ops: []\n",
       "  _required: false\n",
       "  _value: ./checkpoints_single_lstm_42_200_epoch\n",
       "lr_scheduler:\n",
       "  constructor: torch.optim.lr_scheduler.OneCycleLR\n",
       "  params:\n",
       "    anneal_strategy: cos\n",
       "    epochs: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "      _field_type: *id001\n",
       "      _ops: []\n",
       "      _required: false\n",
       "      _value: 100\n",
       "    final_div_factor: 100000\n",
       "    max_lr: 0.001\n",
       "    pct_start: 0.1\n",
       "    steps_per_epoch: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "      _field_type: *id001\n",
       "      _ops: []\n",
       "      _required: false\n",
       "      _value: 604\n",
       "  use_lambda: false\n",
       "model:\n",
       "  first:\n",
       "    backbone:\n",
       "      Resnet34Backbone:\n",
       "        max_pool_stride_1: true\n",
       "        num_layers: 3\n",
       "        pretrained: true\n",
       "      constructor: Resnet34Backbone\n",
       "    decoder:\n",
       "      CTCDecoderModel:\n",
       "        num_classes: *id005\n",
       "        time_feature_count: *id006\n",
       "      constructor: CTCDecoderModel\n",
       "    encoder:\n",
       "      BiLSTMEncoder:\n",
       "        dropout: 0.1\n",
       "        hidden_size: *id007\n",
       "        input_size: *id007\n",
       "        num_layers: 3\n",
       "      TransformerEncoder:\n",
       "        hidden_features: 1024\n",
       "        in_features: *id007\n",
       "        num_heads: 4\n",
       "        num_layers: 4\n",
       "        pe_max_len: 1500\n",
       "      constructor: BiLSTMEncoder\n",
       "  second:\n",
       "    backbone:\n",
       "      Resnet34Backbone:\n",
       "        max_pool_stride_1: true\n",
       "        num_layers: 3\n",
       "        pretrained: true\n",
       "      constructor: Resnet34Backbone\n",
       "    decoder:\n",
       "      CTCDecoderModel:\n",
       "        num_classes: *id005\n",
       "        time_feature_count: *id006\n",
       "      constructor: CTCDecoderModel\n",
       "    encoder:\n",
       "      BiLSTMEncoder:\n",
       "        dropout: 0.1\n",
       "        hidden_size: *id007\n",
       "        input_size: *id007\n",
       "        num_layers: 3\n",
       "      TransformerEncoder:\n",
       "        hidden_features: 1024\n",
       "        in_features: *id007\n",
       "        num_heads: 4\n",
       "        num_layers: 4\n",
       "        pe_max_len: 1500\n",
       "      constructor: BiLSTMEncoder\n",
       "  type: single\n",
       "optimizer:\n",
       "  constructor: torch.optim.AdamW\n",
       "  params:\n",
       "    betas: *id008\n",
       "    lr: 0.0002\n",
       "  weight_decay: 0.01\n",
       "seed: 67\n",
       "training:\n",
       "  batch_size: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "    _field_type: *id001\n",
       "    _ops: []\n",
       "    _required: false\n",
       "    _value: 16\n",
       "  checkpoints_folder: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "    _field_type: *id009\n",
       "    _ops: []\n",
       "    _required: false\n",
       "    _value: ./checkpoints_single_lstm_42_200_epoch\n",
       "  eval_epochs_interval: 3\n",
       "  eval_test_interval: 3\n",
       "  grad_clip_value: 1.0\n",
       "  load_from_checkpoint: true\n",
       "  load_train_state: false\n",
       "  loader_num_workers: 6\n",
       "  num_epochs: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n",
       "    _field_type: *id001\n",
       "    _ops: []\n",
       "    _required: false\n",
       "    _value: 100\n",
       "  snapshot_epochs_interval: 3\n",
       "wandb:\n",
       "  project_name: diploma_dml\n",
       "  resume: false\n",
       "  run_name: __NAME__"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "fe3ad223",
   "metadata": {
    "cellId": "9tbeis7l9hm8bm1d5xfy7e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "os.makedirs(cfg.training.checkpoints_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "6fe475a3",
   "metadata": {
    "cellId": "go9qtj240f86da5fcl3hfl"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#!rm checkpoints_lstm_67_merged_10/model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "8beb6012",
   "metadata": {
    "cellId": "oy8j3suimqsspbmjfwtqof"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "seed_everything(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "5bc9d6e8",
   "metadata": {
    "cellId": "hig1sy1ckys27fapqi9kpl"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#LTRTrainer.merge_model_checkpoints('./checkpoints_single_lstm_67_explore_10', './checkpoints_single_lstm_67', './checkpoints_lstm_67_merged_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "ca00338e",
   "metadata": {
    "cellId": "l9suw4a51r117dnk1qts6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_state.pth\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "!ls checkpoints_lstm_67_merged_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "23a66477",
   "metadata": {
    "cellId": "lhdrvuzy4vxsl8yl3rlk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /tmp/xdg_cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef95cfa71bc4cbeb47316be0a1bbf43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=87319819.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "ltr = LTRTrainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "6bafd008",
   "metadata": {
    "cellId": "68dfqz3s4joyaotb3e71hd"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#ltr.validate(mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "7ae24ca4",
   "metadata": {
    "cellId": "ceok2m4z5na7aohlhtr4uw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/jupyter/work/resources/mutual_htr/mutual_htr/wandb/run-20230622_150111-votx4nvj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlstm_saintgall_6_54_dml_notr_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kafka_zhuk/diploma_dml\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kafka_zhuk/diploma_dml/runs/votx4nvj\u001b[0m\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.87it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.89it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.13it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.12it/s]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.14s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.92it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.89it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.09it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.11it/s]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.14s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.86it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.84it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.12it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.13it/s]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.86it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.83it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.13it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.12it/s]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.14s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.87it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.84it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.10it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.13it/s]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.14s/it]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.14s/it]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.14s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.86it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.86it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.10it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.09it/s]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.15s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.83it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.89it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.11it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.10it/s]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.14s/it]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.14s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.84it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.85it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.11it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.11it/s]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.14s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.10s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.84it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.88it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.10it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.10it/s]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.87it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.82it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.12it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.11it/s]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.85it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.86it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.09it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.09it/s]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.15s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.15s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.82it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.87it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.12it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.14it/s]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.13s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.87it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.84it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.11it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.14it/s]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.14s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.83it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.80it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.11it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.13it/s]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.13s/it]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.15s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.79it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.86it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.09it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.12it/s]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.14s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.88it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.87it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.11it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.09it/s]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.14s/it]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.15s/it]\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.14s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.86it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.84it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.11it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.14it/s]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.11s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.13s/it]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.88it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.90it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.10it/s]\n",
      "100%|██████████| 45/45 [00:14<00:00,  3.11it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: backbone_grad_norm/train ▂▂▂▂▁▁▁▁█▂▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      backbone_lr_0/train ▁▃▆▇██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      backbone_lr_1/train ▁▃▆▇██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch_cer_1/test █▅▄▄▂▃▃▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch_cer_1/valid █▅▄▃▃▃▃▃▂▁▁▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch_cer_2/test █▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch_cer_2/valid █▅▅▃▂▂▃▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch_loss_1/test ▁▃▄▄▅▆▇▆▆▇▆█▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       epoch_loss_1/train ██▆▅▅▄▃▃▃▃▃▂▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       epoch_loss_1/valid ▁▃▄▄▅▆▇▆▆▆▆█▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch_loss_2/test ▁▃▄▅▄▆▇▆▅▆▇█▇▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       epoch_loss_2/train █▇▅▄▄▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       epoch_loss_2/valid ▁▃▄▅▄▆▇▆▅▆▇█▇▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch_loss_kl/train █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          kl_weight/train ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             loss_1/train ▄█▃▅▂▄▃▄▂▆▃▂▃▄▃▃▃▂▂▃▂▂▂▁▂▂▂▃▃▁▂▂▃▄▄▃▂▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             loss_2/train ▇█▃▅▂▃▃▃▃▄▃▂▂▃▃▃▂▂▂▂▂▂▂▁▁▂▂▃▃▁▂▂▂▄▃▃▂▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            loss_kl/train █▃▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               lr_0/train ▁▃▆▇██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               lr_1/train ▁▃▆▇██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    model_grad_norm/train ▆▄▂▃▂▂▂▂█▃▂▂▂▃▂▂▂▁▂▂▁▂▂▁▁▁▂▂▂▁▂▁▂▂▂▃▂▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: backbone_grad_norm/train 0.31337\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      backbone_lr_0/train 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      backbone_lr_1/train 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch_cer_1/test 4.14388\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch_cer_1/valid 4.1718\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch_cer_2/test 4.18903\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch_cer_2/valid 4.13331\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch_loss_1/test 0.42249\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       epoch_loss_1/train 0.02773\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       epoch_loss_1/valid 0.44669\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch_loss_2/test 0.4318\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       epoch_loss_2/train 0.02835\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       epoch_loss_2/valid 0.45182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch_loss_kl/train 0.01676\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          kl_weight/train 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             loss_1/train 0.05202\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             loss_2/train 0.04352\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            loss_kl/train 0.01477\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               lr_0/train 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               lr_1/train 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    model_grad_norm/train 0.22345\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mlstm_saintgall_6_54_dml_notr_2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/kafka_zhuk/diploma_dml/runs/votx4nvj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230622_150111-votx4nvj/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "ltr.train('lstm_saintgall_6_54_dml_notr_2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26bb11a",
   "metadata": {
    "cellId": "evcy0b8ococq6yvcmjcsim"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885e9d5",
   "metadata": {
    "cellId": "iwur2ng4kzk53hbzx57q6o"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "3ad6c332-3486-42e7-aa4d-906f3d8b640c",
  "notebookPath": "mutual_htr/mutual_htr/diploma_ctc_torch.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
